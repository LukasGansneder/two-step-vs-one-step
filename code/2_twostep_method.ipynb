{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-Step: Headlines -> Sentiment (θ̂t) -> Returns regression\n",
    "# Erwartete Spalten in deiner CSV: 'Date', 'Title', 'CP'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- 0) Laden & Vorbereiten ----\n",
    "df = pd.read_csv('\\sp500_headlines_2008_2024.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---- 1) Sentiment pro Headline ----\n",
    "# Primär: VADER; Fallback: sehr einfaches Pos/Neg-Lexikon\n",
    "use_vader = True\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['sent_raw'] = df['Title'].astype(str).map(lambda x: sid.polarity_scores(x)['compound'])\n",
    "except Exception as e:\n",
    "    use_vader = False\n",
    "    pos = set(\"strong beat beats growth surge record upbeat rally gain improve upgrade robust\".split())\n",
    "    neg = set(\"fall falls fell drop drops miss concern concerns probe lawsuit volatility uncertainty downgrade weak\".split())\n",
    "    def lex_score(s: str) -> float:\n",
    "        words = str(s).lower().split()\n",
    "        score = sum(w in pos for w in words) - sum(w in neg for w in words)\n",
    "        return max(-1.0, min(1.0, score/5.0))\n",
    "    df['sent_raw'] = df['Title'].map(lex_score)\n",
    "\n",
    "# ---- 2) Aggregation: mehrere Headlines pro Tag -> ein Tages-Sentiment θ̂t ----\n",
    "# (Durchschnitt ist Standard; Median robuster – wähle was dir passt)\n",
    "daily = (\n",
    "    df.groupby('Date', as_index=False)\n",
    "      .agg(hat_theta=('sent_raw','mean'),\n",
    "           CP=('CP','last'),\n",
    "           Return_simple=('Return_simple','last'),\n",
    "           Return_log=('Return_log','last'),\n",
    "           n_headlines=('Title','size'))\n",
    ")\n",
    "\n",
    "# Optional: Z-Standardisierung des Sentiments (für interpretierbare Koeffizienten)\n",
    "daily['hat_theta_z'] = (daily['hat_theta'] - daily['hat_theta'].mean()) / daily['hat_theta'].std(ddof=0)\n",
    "\n",
    "print(daily.head())\n",
    "\n",
    "# ---- 3) OLS mit Newey-West/HAC-Standardfehlern ----\n",
    "# Basismodell wie bei Tetlock: R_t = γ0 + γ1 * θ̂_t + ε_t\n",
    "def ols_hac(y, X, lags=5):\n",
    "    Xc = sm.add_constant(X)\n",
    "    return sm.OLS(y, Xc, missing='drop').fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n",
    "\n",
    "# a) einfache Rendite\n",
    "mod_simple = ols_hac(daily['Return_simple'], daily['hat_theta_z'], lags=5)\n",
    "print(\"\\n=== Two-Step: Return_simple ~ hat_theta_z (HAC) ===\")\n",
    "print(mod_simple.summary())\n",
    "\n",
    "# b) log-Rendite\n",
    "mod_log = ols_hac(daily['Return_log'], daily['hat_theta_z'], lags=5)\n",
    "print(\"\\n=== Two-Step: Return_log ~ hat_theta_z (HAC) ===\")\n",
    "print(mod_log.summary())\n",
    "\n",
    "# ---- 4) (Optional) Robustness: Lagged Sentiment & Day-of-Week FEs ----\n",
    "daily['hat_theta_z_lag1'] = daily['hat_theta_z'].shift(1)\n",
    "daily['dow'] = daily['Date'].dt.dayofweek  # 0=Mon ... 4=Fri (meist nur Handelstage in deinen Daten)\n",
    "dow_dummies = pd.get_dummies(daily['dow'], prefix='dow', drop_first=True)\n",
    "\n",
    "X_rob = pd.concat([daily[['hat_theta_z','hat_theta_z_lag1']], dow_dummies], axis=1)\n",
    "mod_log_rob = ols_hac(daily['Return_log'], X_rob, lags=5)\n",
    "print(\"\\n=== Robustness: Return_log ~ hat_theta_z + lag1 + DOW FEs (HAC) ===\")\n",
    "print(mod_log_rob.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea54ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
